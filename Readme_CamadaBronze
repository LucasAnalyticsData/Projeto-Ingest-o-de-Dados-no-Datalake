# üß† Guia Explicativo: Boas Pr√°ticas na Camada Bronze

Este documento re√∫ne as boas pr√°ticas aplicadas na **Camada Bronze** do pipeline de dados, com foco em performance, economia de recursos e confiabilidade.

---

## 1. ‚úÖ Leitura eficiente com Auto Loader

### ‚ùå Antes (forma tradicional):
```python
spark.read.format("csv").load("caminho/dados")
```
Essa abordagem:
- L√™ todos os arquivos de uma vez, toda vez;
- N√£o escala bem para grandes volumes;
- N√£o detecta automaticamente novos arquivos (sem reprocessar os antigos).

### ‚úÖ Agora (com Auto Loader):
```python
spark.readStream \
     .format("cloudFiles") \
     .option("cloudFiles.format", "csv") \
     .load(BRONZE_PATH)
```

### üìå Vantagens:
- Detecta automaticamente arquivos novos (sem reler os antigos);
- Ideal para pipelines cont√≠nuos ou agendados;
- Melhor uso de recursos do cluster;
- Suporta grandes volumes com performance.

---

## 2. ‚úÖ Particionamento correto dos dados

### Exemplo:
```python
.write.partitionBy("data_carga")
```

### üß† O que √© `partitionBy`?
√â uma forma de **organizar fisicamente os arquivos no Data Lake**. Por exemplo:
```
bronze/clientes/data_carga=2025-04-14/
bronze/clientes/data_carga=2025-04-15/
```

### üìå Vantagens:
- Spark l√™ apenas o necess√°rio (ex: apenas um m√™s);
- Reduz tempo de leitura e custo computacional;
- Evita leitura desnecess√°ria (√≥timo em grandes volumes).

---

## 3. ‚úÖ Trigger otimizada para controle de recursos

### Exemplo:
```python
.writeStream \
     .trigger(once=True)
```

### üß† Por que isso √© importante?
Spark Structured Streaming normalmente fica monitorando o tempo todo. Isso **consome o cluster**, mesmo sem novos arquivos.

### üìå Com `.trigger(once=True)`:
- Executa apenas uma vez e finaliza o job;
- Ideal para **pipelines batch automatizados**;
- Evita consumo desnecess√°rio de recursos.

---

## 4. ‚úÖ Uso de checkpoint e schema evolution

### Exemplo:
```python
.option("checkpointLocation", checkpoint_path) \
.option("mergeSchema", "true")
```

### üß† Para que serve?
- `checkpointLocation`: salva o **estado atual** da leitura cont√≠nua;
- `mergeSchema`: permite aceitar **novas colunas** sem quebrar o pipeline.

### üìå Vantagens:
- Garante **toler√¢ncia a falhas**;
- Permite **evolu√ß√£o segura** do schema;
- Pipeline mais resiliente em produ√ß√£o.

---

## 5. ‚úÖ Limpeza e tratamento antecipado

### Exemplo:
```python
df = df.dropDuplicates().na.drop()
```

### üß† Explica√ß√£o:
- `dropDuplicates()`: remove registros duplicados;
- `na.drop()`: remove linhas com valores nulos (null).

### üìå Por que fazer isso na Bronze?
- Reduz complexidade nas camadas Silver e Gold;
- Evita erros em joins e m√©tricas erradas;
- Garante que os dados cheguem limpos para an√°lise.

---

## ‚úÖ Resumo Visual

| T√©cnica                       | Benef√≠cio principal                         |
|-------------------------------|----------------------------------------------|
| Auto Loader                   | Leitura escal√°vel e incremental              |
| partitionBy                   | Leitura seletiva, performance e economia     |
| trigger(once=True)            | Controle de execu√ß√£o e uso do cluster        |
| checkpoint + mergeSchema      | Toler√¢ncia a falhas e schema flex√≠vel        |
| dropDuplicates + na.drop()    | Dados limpos desde a origem                  |

---

> _"Engenharia de dados come√ßa na Bronze: quanto melhor a base, mais poderosa ser√° a entrega."_

